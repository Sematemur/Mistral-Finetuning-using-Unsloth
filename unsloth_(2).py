# -*- coding: utf-8 -*-
"""unsloth_(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uJlbv1wWikMSfNb_w7elgg8dH0Tt48zA

kod yardımı için bu siteyi kullandım:https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb

verisetini buradan aldım. İlkokul matematik soruları içeren bir veriseti ile modeli finetune ettim https://huggingface.co/datasets/openai/gsm8k.
modeli unsloth framework'unu kullanarak finetune ettim. Unsloth ile yapıldığında daha hızlı bir şekilde LLM modelini finetune edebiliyorsunuz.
Modeli 25 epoch olacak şekilde finetune ettim. Daha sonra ise modeli huggingface hesabıma hem normal olarak hem de GGUF şekli ile push ettim. Modelden ollama ile local olarak inference yapmak için gguf şeklinde olması lazım.
Push edilen modeli kullanmak için ollamayı indirip ,terminalden
"ollama run hf.co/sematemur/openai_gsm8k_finetune_mistral7b_ "yazarak çalıştırabilirsiniz.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # Otomatik cihaz haritalama
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 8,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

alpaca_prompt = """Below is a prompt paired with an input and an output. Write a response that appropriately completes the request.

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    inputs       = examples["question"]
    outputs      = examples["answer"]
    texts = []
    for input, output in zip(inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

from datasets import load_dataset
ds = load_dataset("openai/gsm8k","main",split="train")
dataset = ds.map(formatting_prompts_func, batched = True,)

from trl import SFTConfig, SFTTrainer
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = SFTConfig(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 25,
        learning_rate = 2e-4,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)

trainer_stats = trainer.train()

FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "what is 5*9", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)

model.push_to_hub("sematemur/openai_gsm8k_finetuned_mistral7b", token = ":)") # Online saving
 tokenizer.push_to_hub("sematemur/openai-gsm8k_finetuned_mistral7b", token = ":)") # Online saving

# To address OutOfMemoryError, try saving directly to an 8-bit GGUF model.
# This requires less memory than merging to 16-bit.
model.save_pretrained_gguf("model", tokenizer, quantization_method = "q8_0")

model.push_to_hub_gguf("sematemur/openai_gsm8k_finetune_mistral7b_",tokenizer,token=":)")

